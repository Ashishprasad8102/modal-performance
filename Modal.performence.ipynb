{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56ec82-c3c2-4aad-ad6c-24bbe444a4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Overfitting:  When the model fits the training data too well and performance poorly on new data.\n",
    "   underfitting: When the modal fits the training data poorly and performace poorly on new data.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b4cad-9a4f-48a3-9804-2c83a642fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.we can reduse the overfitting ,to train the modal with more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51a1f2-fd3e-43ba-a953-812f546853fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.when the modal fits the training data poorly and performace poorly on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a313aa6-fe9c-4295-ab14-f04ab2edf548",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship\n",
    "between the complexity of a model and its ability to generalize to new data. In essence, it is a tradeoff\n",
    "between the model's ability to fit the training data well (low bias) and its ability to perform well on\n",
    "unseen data (low variance).\n",
    "\n",
    "Bias refers to the error that is introduced by approximating a real-world problem with a simplified model.\n",
    "In other words, it is the difference between the expected predictions of the model and the true values of\n",
    "the data. A high bias model is typically oversimplified and fails to capture the underlying patterns in \n",
    "the data, resulting in poor performance on both the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dc490-3cfc-49c3-81c5-ff25dd134d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "5.Overfitting and underfitting are common problems in machine learning models. Overfitting occurs when the\n",
    "model is too complex and fits the training data too well, but fails to generalize to new, unseen data. \n",
    "Underfitting, on the other hand, occurs when the model is too simple and fails to capture the underlying\n",
    "patterns in the data, resulting in poor performance on both the training and testing data.\n",
    "\n",
    "Here are some common methods for detecting overfitting and underfitting in machine learning models:\n",
    "\n",
    "Training and testing error: One of the simplest ways to detect overfitting and underfitting is by\n",
    "comparing the training and testing error. If the training error is low, but the testing error is\n",
    "high, the model may be overfitting. If both the training and testing error are high, the model\n",
    "may be underfitting.\n",
    "\n",
    "Learning curves: Learning curves are plots of the training and testing error as a function of the\n",
    "number of training examples or epochs. If the training error decreases but the testing error \n",
    "plateaus or starts to increase, the model may be overfitting. If both the training and testing \n",
    "error are high and don't improve with more data or epochs, the model may be underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368db49-4d63-477d-8aca-4959b27a6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "6.Bias and variance are two important concepts in machine learning that describe different aspects of model\n",
    "performance.\n",
    "\n",
    "Bias refers to the error that is introduced by approximating a real-world problem with a simplified model. \n",
    "It is a measure of how well the model fits the training data. A high bias model is typically oversimplified\n",
    "and fails to capture the underlying patterns in the data, resulting in poor performance on both the training \n",
    "and test sets. In other words, a high bias model underfits the data.\n",
    "\n",
    "Variance, on the other hand, refers to the amount by which the predictions of a model would change if it\n",
    "were trained on a different dataset. It is a measure of how sensitive the model is to fluctuations in the\n",
    "training data. A high variance model is often overly complex and can fit the noise in the training data,\n",
    "resulting in good performance on the training set but poor performance on the test set. In other words, a\n",
    "high variance model overfits the data.\n",
    "\n",
    "A high bias model is usually associated with a low degree of freedom and a low variance, while a high \n",
    "variance model is associated with a high degree of freedom and a low bias. Therefore, there is a tradeoff\n",
    "between bias and variance that needs to be balanced in order to achieve optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64464403-08a3-48aa-b27c-8c662810acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.Regularization is a technique used in machine learning to prevent overfitting of models. Overfitting\n",
    "occurs when a model fits too closely to the training data and captures noise rather than the underlying \n",
    "patterns in the data. Regularization works by adding a penalty term to the objective function of the\n",
    "model, which discourages the model from fitting the noise in the data and encourages it to generalize \n",
    "better to new, unseen data.\n",
    "\n",
    "There are several common regularization techniques used in machine learning, including:\n",
    "\n",
    "L1 Regularization (Lasso Regression): In L1 regularization, a penalty term is added to the objective function of the model, which is proportional to the absolute value of the coefficients of the model. This results in some coefficients becoming exactly zero, effectively performing feature selection and reducing the complexity of the model. L1 regularization is particularly useful when dealing with high-dimensional datasets where only a small subset of features may be relevant.\n",
    "\n",
    "L2 Regularization (Ridge Regression): In L2 regularization, a penalty term is added to the objective function of the model, which is proportional to the square of the coefficients of the model. This results in the coefficients being shrunk towards zero, but none of them exactly become zero, so all features are retained. L2 regularization is useful for preventing overfitting in linear regression models, as it reduces the magnitude of the coefficients and makes them less sensitive to noise in the data.\n",
    "\n",
    "Elastic Net Regularization: Elastic net regularization is a combination of L1 and L2 regularization. It adds both L1 and L2 penalty terms to the objective function of the model, resulting in a combination of the benefits of both regularization techniques. It can be particularly useful when dealing with high-dimensional datasets where both feature selection and coefficient shrinkage are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f18255-deb4-4f90-8652-56ddd587e647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
